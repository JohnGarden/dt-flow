d_model: 128
n_layers: 6
n_heads: 8
d_ff: 1024
dropout: 0.1


vocab:
  n_actions: 2   # apenas {0,1}; 'wait' fica só na decodificação
  start_action_id: 3


# Dimensão alvo do "token de estado" após tokenização/AE
embeddings:
  state_embed_dim: 128   # igual ao state_dim já previsto
  rtg_dim: 64
  time_dim: 32

loss:
  label_smoothing: 0.0
  class_weights: [1.0, 3.0, 1.0]   # pesa mais a classe positiva (malicious)
  
regularization:
  label_smoothing: 0.0


# Especificações de categóricas
categorical:
  cols: ["proto", "service", "state"]
  # regra do tamanho do embedding: sqrt, log ou fixed
  embed_rule: "sqrt"   # sqrt(n_cat)*2, clip [4, 32]
  fixed_dim: 16        # usado se embed_rule=="fixed"

# Autoencoder
autoencoder:
  use: false
  bottleneck_dim: 32
  hidden: [256, 128]
  dropout: 0.0